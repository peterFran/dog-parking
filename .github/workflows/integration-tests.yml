name: Integration Tests with Ephemeral Environment

on:
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]
  push:
    branches: [main]

permissions:
  contents: read
  pull-requests: write
  issues: write

env:
  AWS_REGION: us-east-1
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  integration-tests:
    runs-on: ubuntu-latest
    environment: staging
    
    env:
      # Create unique stack name for this test run
      TEST_STACK_NAME: dog-care-test-${{ github.run_id }}
      TEST_PROJECT_ID: demo-dog-care
      FIREBASE_AUTH_EMULATOR_HOST: localhost:9099
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Setup Node.js for Firebase CLI
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Install SAM CLI
        uses: aws-actions/setup-sam@v2
        with:
          use-installer: true
      
      - name: Install Firebase CLI
        run: npm install -g firebase-tools
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
      
      - name: Build SAM application
        run: sam build --use-container
      
      - name: Deploy ephemeral test environment
        run: |
          echo "Deploying test stack: ${{ env.TEST_STACK_NAME }}"
          
          # Use the same bucket pattern as staging deployment
          # Based on staging logs: dog-care-sam-deployments-20250713-51
          BUCKET_NAME="dog-care-sam-deployments-$(date +%Y%m%d)-${{ github.run_number }}"
          echo "📦 Using S3 bucket: $BUCKET_NAME"

          # Try to reuse an existing bucket from staging first
          EXISTING_BUCKET_BASE="dog-care-sam-deployments-$(date +%Y%m%d)"
          BUCKET_FOUND=false

          # Look for any existing bucket with today's date
          for i in {1..100}; do
            TEST_BUCKET="${EXISTING_BUCKET_BASE}-${i}"
            if aws s3 ls "s3://$TEST_BUCKET" >/dev/null 2>&1; then
              echo "✅ Found existing bucket: $TEST_BUCKET"
              BUCKET_NAME="$TEST_BUCKET"
              BUCKET_FOUND=true
              break
            fi
          done

          # Create bucket if none found
          if [ "$BUCKET_FOUND" = false ]; then
            echo "📦 Creating new S3 bucket: $BUCKET_NAME"
            aws s3 mb "s3://$BUCKET_NAME" --region ${{ env.AWS_REGION }}
            if [ $? -eq 0 ]; then
              echo "✅ Successfully created bucket: $BUCKET_NAME"
            else
              echo "❌ Failed to create bucket: $BUCKET_NAME"
              exit 1
            fi
          fi

          echo "📦 Final bucket choice: $BUCKET_NAME"
          
          sam deploy \
            --template-file .aws-sam/build/template.yaml \
            --stack-name ${{ env.TEST_STACK_NAME }} \
            --parameter-overrides "Environment=test GoogleProjectId=${{ env.TEST_PROJECT_ID }}" \
            --capabilities CAPABILITY_IAM \
            --region ${{ env.AWS_REGION }} \
            --no-fail-on-empty-changeset \
            --no-confirm-changeset \
            --s3-bucket $BUCKET_NAME \
            --debug
      
      - name: Get API endpoint from stack
        id: get-api-url
        run: |
          API_URL=$(aws cloudformation describe-stacks \
            --stack-name ${{ env.TEST_STACK_NAME }} \
            --query 'Stacks[0].Outputs[?OutputKey==`DogCareApiUrl`].OutputValue' \
            --output text \
            --region ${{ env.AWS_REGION }})
          echo "API_URL=$API_URL" >> $GITHUB_OUTPUT
          echo "Test API URL: $API_URL"
      
      - name: Start Firebase Auth Emulator
        run: |
          echo "Starting Firebase Auth emulator..."
          firebase emulators:start --only auth --project ${{ env.TEST_PROJECT_ID }} &
          
          # Wait for emulator to be ready
          echo "Waiting for Firebase emulator to start..."
          timeout 60 bash -c 'until curl -s http://localhost:9099/ > /dev/null; do sleep 2; done'
          echo "Firebase emulator is ready"
      
      - name: Wait for API to be ready
        run: |
          echo "Waiting for API to be ready..."
          timeout 300 bash -c 'until curl -s "${{ steps.get-api-url.outputs.API_URL }}/venues" > /dev/null; do sleep 10; done'
          echo "API is ready"
      
      - name: Run integration tests
        env:
          API_BASE_URL: ${{ steps.get-api-url.outputs.API_URL }}
          FIREBASE_AUTH_EMULATOR_HOST: ${{ env.FIREBASE_AUTH_EMULATOR_HOST }}
        run: |
          echo "Running integration tests against: $API_BASE_URL"
          mkdir -p test-results

          # Run tests with detailed output and capture results
          python -m pytest tests/integration/test_api_with_firebase_emulator.py \
            -v --tb=short \
            --junitxml=test-results/integration-test-results.xml \
            --capture=no \
            --strict-markers \
            --disable-warnings \
            2>&1 | tee test-results/integration-test-output.txt

          TEST_EXIT_CODE=$?
          echo "INTEGRATION_TEST_EXIT_CODE=$TEST_EXIT_CODE" >> $GITHUB_ENV

          # Parse results and fail if all tests were skipped
          TOTAL_TESTS=$(grep -o "collected [0-9]* items" test-results/integration-test-output.txt | grep -o "[0-9]*" || echo "0")
          SKIPPED_TESTS=$(grep -o "[0-9]* skipped" test-results/integration-test-output.txt | grep -o "^[0-9]*" || echo "0")
          PASSED_TESTS=$(grep -o "[0-9]* passed" test-results/integration-test-output.txt | grep -o "^[0-9]*" || echo "0")
          FAILED_TESTS=$(grep -o "[0-9]* failed" test-results/integration-test-output.txt | grep -o "^[0-9]*" || echo "0")
          ERROR_TESTS=$(grep -o "[0-9]* error" test-results/integration-test-output.txt | grep -o "^[0-9]*" || echo "0")

          echo "TOTAL_TESTS=$TOTAL_TESTS" >> $GITHUB_ENV
          echo "PASSED_TESTS=$PASSED_TESTS" >> $GITHUB_ENV
          echo "FAILED_TESTS=$FAILED_TESTS" >> $GITHUB_ENV
          echo "SKIPPED_TESTS=$SKIPPED_TESTS" >> $GITHUB_ENV
          echo "ERROR_TESTS=$ERROR_TESTS" >> $GITHUB_ENV

          echo "📊 Test Results: $TOTAL_TESTS total, $PASSED_TESTS passed, $FAILED_TESTS failed, $SKIPPED_TESTS skipped, $ERROR_TESTS errors"

          # Fail the workflow for any of these conditions:
          # 1. All tests were skipped (indicates setup failure)
          # 2. pytest exited with non-zero code
          # 3. Any tests failed
          # 4. Any tests had errors
          if [ "$TOTAL_TESTS" -gt 0 ] && [ "$SKIPPED_TESTS" -eq "$TOTAL_TESTS" ]; then
            echo "❌ All tests were skipped - this indicates a setup failure"
            echo "INTEGRATION_TEST_STATUS=SKIPPED_ALL" >> $GITHUB_ENV
            exit 1
          elif [ "$TEST_EXIT_CODE" -ne 0 ]; then
            echo "❌ pytest exited with code $TEST_EXIT_CODE"
            echo "INTEGRATION_TEST_STATUS=FAILED" >> $GITHUB_ENV
            exit 1
          elif [ "$FAILED_TESTS" -gt 0 ]; then
            echo "❌ $FAILED_TESTS test(s) failed"
            echo "INTEGRATION_TEST_STATUS=FAILED" >> $GITHUB_ENV
            exit 1
          elif [ "$ERROR_TESTS" -gt 0 ]; then
            echo "❌ $ERROR_TESTS test(s) had errors"
            echo "INTEGRATION_TEST_STATUS=FAILED" >> $GITHUB_ENV
            exit 1
          else
            echo "✅ All tests passed successfully"
            echo "INTEGRATION_TEST_STATUS=PASSED" >> $GITHUB_ENV
          fi

          # Extract detailed test summary for PR comment
          echo "=== Integration Test Results ===" > test-results/test-summary.txt
          grep -E "^tests/.*::" test-results/integration-test-output.txt >> test-results/test-summary.txt || echo "No individual test results found" >> test-results/test-summary.txt
          echo "" >> test-results/test-summary.txt
          tail -3 test-results/integration-test-output.txt >> test-results/test-summary.txt || echo "No final summary found" >> test-results/test-summary.txt
      
      - name: Cleanup - Stop Firebase emulator
        if: always()
        run: |
          echo "Stopping Firebase emulator..."
          pkill -f "firebase emulators" || true
      
      - name: Cleanup - Delete test stack
        if: always()
        run: |
          echo "Deleting test stack: ${{ env.TEST_STACK_NAME }}"
          aws cloudformation delete-stack \
            --stack-name ${{ env.TEST_STACK_NAME }} \
            --region ${{ env.AWS_REGION }}

          echo "Waiting for stack deletion to complete..."
          aws cloudformation wait stack-delete-complete \
            --stack-name ${{ env.TEST_STACK_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --cli-read-timeout 0 \
            --cli-connect-timeout 60 || true

          echo "Stack deletion completed"

      - name: Cleanup - Delete S3 bucket
        if: always()
        run: |
          BUCKET_NAME="dog-care-sam-deployments-$(date +%Y%m%d)-${{ github.run_number }}"
          echo "Cleaning up S3 bucket: $BUCKET_NAME"

          # Empty bucket first (required before deletion)
          aws s3 rm "s3://$BUCKET_NAME" --recursive || true

          # Delete the bucket
          aws s3 rb "s3://$BUCKET_NAME" || true

          echo "S3 bucket cleanup completed"
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results-${{ github.run_id }}
          path: |
            test-results/
            htmlcov/
          retention-days: 7

      - name: Comment integration test results on PR
        uses: actions/github-script@v7
        if: always() && github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            const testStatus = '${{ env.INTEGRATION_TEST_STATUS }}' || 'UNKNOWN';
            const passedCount = '${{ env.PASSED_TESTS }}' || '0';
            const failedCount = '${{ env.FAILED_TESTS }}' || '0';
            const skippedCount = '${{ env.SKIPPED_TESTS }}' || '0';
            const errorCount = '${{ env.ERROR_TESTS }}' || '0';
            const totalCount = '${{ env.TOTAL_TESTS }}' || '0';

            let statusEmoji, statusMessage;
            if (testStatus === 'PASSED') {
              statusEmoji = '✅ PASSED';
              statusMessage = '🎉 All integration tests passed! The API is working correctly with real AWS infrastructure.';
            } else if (testStatus === 'FAILED') {
              statusEmoji = '❌ FAILED';
              statusMessage = '❌ Some integration tests failed. Please check the detailed results below and fix the issues.';
            } else if (testStatus === 'SKIPPED_ALL') {
              statusEmoji = '⚠️ SETUP FAILED';
              statusMessage = '⚠️ All tests were skipped due to setup failure. This usually indicates Firebase Auth emulator connectivity issues.';
            } else {
              statusEmoji = '❓ UNKNOWN';
              statusMessage = '❓ Test status could not be determined. Please check the workflow logs.';
            }

            // Try to read detailed test summary
            let testDetails = '';
            try {
              testDetails = fs.readFileSync('test-results/test-summary.txt', 'utf8');
              testDetails = `
            ### 📊 Detailed Test Results:
            \`\`\`
            ${testDetails.trim()}
            \`\`\`
            `;
            } catch (error) {
              testDetails = '\n*Detailed test results not available*\n';
            }

            const body = `## 🚀 Integration Test Results

            **Status:** ${statusEmoji}
            **Test Summary:** ${passedCount} passed, ${failedCount} failed, ${skippedCount} skipped, ${errorCount} errors (${totalCount} total)
            **Commit:** ${{ github.sha }}
            **Environment:** Ephemeral AWS stack (\`${{ env.TEST_STACK_NAME }}\`)

            ### What was tested:
            - 🚀 **AWS Deployment**: SAM stack deployment to us-east-1
            - 🔥 **Firebase Auth**: Authentication emulator integration
            - 🧪 **API Integration**: End-to-end API testing with real AWS services
            - 📊 **Test Coverage**: Full integration test suite
            - 🧹 **Resource Cleanup**: Automatic stack and S3 bucket cleanup

            ${testDetails}

            ### Test Environment:
            - **Stack Name**: \`${{ env.TEST_STACK_NAME }}\`
            - **Firebase Project**: \`${{ env.TEST_PROJECT_ID }}\`
            - **AWS Region**: \`${{ env.AWS_REGION }}\`
            - **API Endpoint**: ${testStatus === 'PASSED' ? 'Deployed and tested successfully' : 'Deployed but tests had issues'}
            - **Test Artifacts**: [Download results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})

            ${statusMessage}

            **Full logs:** [View workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            })